{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb076609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numbers\n",
    "import nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b9989d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def remove_md(string):\n",
    "    \"\"\"\n",
    "    Removes angled brackets (< & >) and the markdown formatting inside them from strings. Should \n",
    "    remove anything between two angled brackets, anything before a close bracket if there is only a\n",
    "    close bracket, and anything after an open bracket if there is only an open bracket\n",
    "    \n",
    "    Input : str\n",
    "    output : str\n",
    "    \"\"\"\n",
    "\n",
    "    # if both an open and close angle brackets are in the string\n",
    "    if ('<' in string) and ('>' in string):\n",
    "        open_brac = string.find('<')\n",
    "        close_brac = string.find('>')\n",
    "        # if there is something inside the brackets, drop that including the brackets\n",
    "        if open_brac < close_brac:\n",
    "            string = string.replace(string[open_brac : close_brac + 1],\"\")\n",
    "        # if there isn't anything in between them, drop everything before and after\n",
    "        else:\n",
    "            string = string.replace(string[:close_brac+1],\"\")\n",
    "            string = string.replace(string[open_brac:],\"\")\n",
    "        # use recursion to fix any instances where there are multiple opens and closes\n",
    "        return remove_md(string)\n",
    "    elif ('<' in string):\n",
    "        open_brac = string.find('<')\n",
    "        string = string.replace(string[open_brac:],\"\")\n",
    "        return remove_md(string)\n",
    "    elif ('>' in string):\n",
    "        close_brac = string.find('>')\n",
    "        string = string.replace(string[:close_brac+1],\"\")\n",
    "        return remove_md(string)\n",
    "    else:\n",
    "        return string\n",
    "\n",
    "\n",
    "def drop_words(word_list):\n",
    "    \"\"\"\n",
    "    Removes stop-words or words that don't really add any meaning so we want to remove to limit\n",
    "    the number of features in our final data set.\n",
    "\n",
    "    Input :\n",
    "        word_list (list of str)\n",
    "    Output :\n",
    "        list of str\n",
    "    \"\"\"\n",
    "\n",
    "    wl = list(word_list)\n",
    "    stop_words = ['this', 'that','we', 'an','be', 'from', 'you', 'for', 'of', 'with', 'is', 'in',\n",
    "                  'to', 'a','the', 'can','things','also', 'my','us', 'and', 'are', 'your', 'w/',\n",
    "                  'has', 'on', 'have', 'or', '&', 'will', 'as', 'at', 'it', '-', 'so', 'â€¢', 'i',\n",
    "                  '+','eg', '|', '/']\n",
    "    \n",
    "    for sw in stop_words:\n",
    "        if sw in wl:\n",
    "            wl.remove(sw)\n",
    "    return wl\n",
    "\n",
    "\n",
    "def ordered_unique(seq):\n",
    "    \"\"\"\n",
    "    Takes the unique values of a list, but unlike np.unique, it returns those unique values in the\n",
    "    same order in which the first occurence appears so ordered_unique([1,1,2,4,3,4]) = [1,2,4,3]\n",
    "    \n",
    "    Input : \n",
    "        seq (iterable) \n",
    "\n",
    "    Output : \n",
    "        list\n",
    "    \"\"\"\n",
    "\n",
    "    seen = set()\n",
    "    seen_add = seen.add\n",
    "    return [x for x in seq if not (x in seen or seen_add(x))]\n",
    "\n",
    "\n",
    "def get_dictionary(load=True, text=None, n_words=4000, overWrite=False, fileDir=\\\n",
    "        '/Users/ckrasnia/Documents/application_materials/rental_data/dictionary.pkl'):\n",
    "    \"\"\"\n",
    "    Retrieves a dictionary of unnique words, where each word is a key and each key is assigned a unique integer. Default\n",
    "    is to load a previously saved file, but to generate a new dictionary, set load=False and provide a text\n",
    "    \n",
    "    Input :\n",
    "        load (bool) defaults to True, if the dictionary should be loade from fileDir\n",
    "        text (list of strings) a list of strings from which to generate the dictionary\n",
    "        n_words (int) number of words to include in the dictionary\n",
    "        overWrite (bool) only applies if text is provided, determines if the dictionary should be saved\n",
    "        fileDir (str) the file that contains the dictionary / where the dictionary will be saved if overWrite=True\n",
    "        \n",
    "    Returns : \n",
    "        a dictionary where there are n_words keys which are strings and the values are a unique integer assigned\n",
    "        according to the frequency of the words with 1 = most frequent. 0 is reserved for words not in the dictionary\n",
    "    \"\"\"\n",
    "    \n",
    "    if text != None:\n",
    "        print('creating new dictionary...')\n",
    "        u,cts = np.unique(text, return_counts=True)\n",
    "        words = list(u[np.argsort(cts)[-n_words:]])\n",
    "        words.reverse()# reverse so that 1 is the most frequent word\n",
    "        vals = np.arange(1,n_words+1) # reserve 0 for words that are not in the top 4000\n",
    "        dictionary = {word:val for word, val in zip(words,vals)}\n",
    "        \n",
    "        if overWrite:\n",
    "            f = open(fileDir,\"wb\")\n",
    "            pickle.dump(dictionary,f)\n",
    "            print('new dictionary saved to {}'.format(fileDir))\n",
    "            f.close()\n",
    "    elif load: # load file if thats what we want\n",
    "        print('loading pregenerated dictionary')\n",
    "        f = open(fileDir,\"rb\")\n",
    "        dictionary = pickle.load(f)\n",
    "        f.close()\n",
    "\n",
    "    else:\n",
    "        raise Exception('dictionary must either be loaded with load=True or created by providing \\\n",
    "            a list of strings to the text argument')\n",
    "        \n",
    "    return dictionary\n",
    "\n",
    "\n",
    "def clean_text(column : pd.Series):\n",
    "    \"\"\"\n",
    "    written to clean up the text data to translate it into numerical values for input to a model.\n",
    "    steps: \n",
    "    1. changes nan values to empty strings\n",
    "    2. lowercases all text\n",
    "    3. removes markdown formatting\n",
    "    4. removes special characters\n",
    "    5. splits into a list of words\n",
    "    6. drops a list of stopwords to pare down to a set of only meaningful words\n",
    "    7. stems words with the porter stemming algorithm to remove endings to reduce the number of\n",
    "        unique words\n",
    "    8. only keeps the unique words of the list, while preserving order\n",
    "    \n",
    "    Input:\n",
    "        column (pd.Series) with each row containing a string containing with multiple words\n",
    "        \n",
    "    Output:\n",
    "        (pd.Series) the cleaned column with a list of unique words in each row\n",
    "    \"\"\"\n",
    "    column[column.isna()] = ''\n",
    "    #look at the original distribution of how many words there are per description\n",
    "    descript_words = column.str.lower()\n",
    "\n",
    "    # first start by getting rid of ugly formatting \n",
    "    descript_words = descript_words.apply(remove_md)\n",
    "    #also going to want to remove some punctuation\n",
    "    descript_words = descript_words.str.replace(\"[():!.,?]\",\"\")\n",
    "    descript_words = descript_words.str.split()\n",
    "\n",
    "    # get rid of meaningless stop words\n",
    "    descript_words = descript_words.apply(drop_words)\n",
    "\n",
    "    # lematize words so that to reduce unique words \n",
    "    # this is pretty time intensive due to the loop over the list within each of the rows\n",
    "    porter = PorterStemmer()\n",
    "    descript_words = descript_words.map(lambda x: [porter.stem(y) for y in x])\n",
    "\n",
    "    # now I want unique words within each description\n",
    "    descript_words = descript_words.apply(ordered_unique)\n",
    "\n",
    "    # sometimes stop-words reappear after lematization, so try dropping them again\n",
    "    descript_words = descript_words.apply(drop_words)\n",
    "    return descript_words\n",
    "\n",
    "\n",
    "def translate(word_list, dictionary):\n",
    "    \"\"\"\n",
    "    A lossy conversion of words to numbers or numbers back to words. if word_list contains strings,\n",
    "    it will translate to using the dictionary, if it is ints, it will translate to strings using\n",
    "    the reverse translation. For words that are not in the dictionary, it will be mapped to None.\n",
    "    for the reverse translation, 0 gets mapped to None\n",
    "    \n",
    "    Input : \n",
    "        word_list (list of int or str) list that you want translated\n",
    "        dictionary (dict with str as keys mapped to a unique int) used to translate between words\n",
    "            and ints\n",
    "        \n",
    "    Returns :\n",
    "        a translated version of the list, the same length as the input word_list\n",
    "    \"\"\"\n",
    "\n",
    "    # catch if there is no description\n",
    "    try:\n",
    "        if len(word_list) == 0:\n",
    "            translated = np.array([0])\n",
    "\n",
    "        \n",
    "        # translate to integers\n",
    "        elif type(word_list[0]) == str:\n",
    "            translated = np.array([*map(dictionary.get, word_list)])\n",
    "            translated[translated==None] = 0\n",
    "\n",
    "        # translate to strings\n",
    "        elif np.any([isinstance(x, numbers.Number) for x in word_list]):\n",
    "            inverse_dict = {v: k for k, v in dictionary.items()}\n",
    "            translated = np.array([*map(inverse_dict.get, word_list)])\n",
    "            \n",
    "        # if a list of Nones, return self as thats the best translation we can do\n",
    "        elif (np.all([x == None for x in word_list])):\n",
    "            translated = np.zeros(len(word_list))\n",
    "            \n",
    "    except TypeError: # catches the case that there is only a None\n",
    "        translated = np.array([0])\n",
    "        \n",
    "    return translated\n",
    "\n",
    "def preprocess_text(column, dictionary=None, return_dict = False):\n",
    "    \"\"\"\n",
    "    Applies both the clean_column function and translate function to yield a numerical series\n",
    "    \n",
    "    Input : \n",
    "        column (pd.Series) a column with rows containing strings, most should have multiple words\n",
    "        dictionary (None or dict) if None, a dictionary is created from the words in the column and\n",
    "            the column is translated with that new dictionary, else a dictionary with words as keys\n",
    "            and unique integers as values\n",
    "        return_dict (bool) if true, the dictionary will also be returned\n",
    "        \n",
    "    Returns : \n",
    "        (pd.Series) the same length as the column input with a 1d array of integers in each row who's values are mapped\n",
    "            to words using the dictionary either provided or generated here. \n",
    "        [optional] \n",
    "        (dict) a dictionary assigning words to unique integers based on their frequency, see get_dictionary for details\n",
    "    \"\"\"\n",
    "    \n",
    "    print('cleaning the text...')\n",
    "    \n",
    "    cleaned_column = clean_text(column)\n",
    "    if dictionary == None:\n",
    "        n = 50000\n",
    "        idx = np.random.choice(np.arange(len(cleaned_column)),n)\n",
    "        word_list = [w for j in cleaned_column.iloc[idx] for w in j]\n",
    "        dictionary = get_dictionary(text=word_list)\n",
    "        \n",
    "    print('applying the dictionary')\n",
    "    \n",
    "    if return_dict:\n",
    "        return cleaned_column.apply(translate,args=(dictionary,)), dictionary\n",
    "    \n",
    "    return cleaned_column.apply(translate,args=(dictionary,))\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "\"\"\"Porter Stemming Algorithm\n",
    "This is the Porter stemming algorithm, ported to Python from the\n",
    "version coded up in ANSI C by the author. It may be be regarded\n",
    "as canonical, in that it follows the algorithm presented in\n",
    "\n",
    "Porter, 1980, An algorithm for suffix stripping, Program, Vol. 14,\n",
    "no. 3, pp 130-137,\n",
    "\n",
    "only differing from it at the points maked --DEPARTURE-- below.\n",
    "\n",
    "See also http://www.tartarus.org/~martin/PorterStemmer\n",
    "\n",
    "The algorithm as described in the paper could be exactly replicated\n",
    "by adjusting the points of DEPARTURE, but this is barely necessary,\n",
    "because (a) the points of DEPARTURE are definitely improvements, and\n",
    "(b) no encoding of the Porter stemmer I have seen is anything like\n",
    "as exact as this version, even with the points of DEPARTURE!\n",
    "\n",
    "Vivake Gupta (v@nano.com)\n",
    "\n",
    "Release 1: January 2001\n",
    "\n",
    "Further adjustments by Santiago Bruno (bananabruno@gmail.com)\n",
    "to allow word input not restricted to one word per line, leading\n",
    "to:\n",
    "\n",
    "release 2: July 2008\n",
    "\n",
    "Modified by CSK to make it more user friendly for my purpose, changed the stem method\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "\n",
    "class PorterStemmer:\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"The main part of the stemming algorithm starts here.\n",
    "        b is a buffer holding a word to be stemmed. The letters are in b[k0],\n",
    "        b[k0+1] ... ending at b[k]. In fact k0 = 0 in this demo program. k is\n",
    "        readjusted downwards as the stemming progresses. Zero termination is\n",
    "        not in fact used in the algorithm.\n",
    "\n",
    "        Note that only lower case sequences are stemmed. Forcing to lower case\n",
    "        should be done before stem(...) is called.\n",
    "        \"\"\"\n",
    "\n",
    "        self.b = \"\"  # buffer for word to be stemmed\n",
    "        self.k = 0\n",
    "        self.k0 = 0\n",
    "        self.j = 0   # j is a general offset into the string\n",
    "\n",
    "    def cons(self, i):\n",
    "        \"\"\"cons(i) is TRUE <=> b[i] is a consonant.\"\"\"\n",
    "        if self.b[i] == 'a' or self.b[i] == 'e' or self.b[i] == 'i' or self.b[i] == 'o' or self.b[i] == 'u':\n",
    "            return 0\n",
    "        if self.b[i] == 'y':\n",
    "            if i == self.k0:\n",
    "                return 1\n",
    "            else:\n",
    "                return (not self.cons(i - 1))\n",
    "        return 1\n",
    "\n",
    "    def m(self):\n",
    "        \"\"\"m() measures the number of consonant sequences between k0 and j.\n",
    "        if c is a consonant sequence and v a vowel sequence, and <..>\n",
    "        indicates arbitrary presence,\n",
    "\n",
    "           <c><v>       gives 0\n",
    "           <c>vc<v>     gives 1\n",
    "           <c>vcvc<v>   gives 2\n",
    "           <c>vcvcvc<v> gives 3\n",
    "           ....\n",
    "        \"\"\"\n",
    "        n = 0\n",
    "        i = self.k0\n",
    "        while 1:\n",
    "            if i > self.j:\n",
    "                return n\n",
    "            if not self.cons(i):\n",
    "                break\n",
    "            i = i + 1\n",
    "        i = i + 1\n",
    "        while 1:\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "            n = n + 1\n",
    "            while 1:\n",
    "                if i > self.j:\n",
    "                    return n\n",
    "                if not self.cons(i):\n",
    "                    break\n",
    "                i = i + 1\n",
    "            i = i + 1\n",
    "\n",
    "    def vowelinstem(self):\n",
    "        \"\"\"vowelinstem() is TRUE <=> k0,...j contains a vowel\"\"\"\n",
    "        for i in range(self.k0, self.j + 1):\n",
    "            if not self.cons(i):\n",
    "                return 1\n",
    "        return 0\n",
    "\n",
    "    def doublec(self, j):\n",
    "        \"\"\"doublec(j) is TRUE <=> j,(j-1) contain a double consonant.\"\"\"\n",
    "        if j < (self.k0 + 1):\n",
    "            return 0\n",
    "        if (self.b[j] != self.b[j-1]):\n",
    "            return 0\n",
    "        return self.cons(j)\n",
    "\n",
    "    def cvc(self, i):\n",
    "        \"\"\"cvc(i) is TRUE <=> i-2,i-1,i has the form consonant - vowel - consonant\n",
    "        and also if the second c is not w,x or y. this is used when trying to\n",
    "        restore an e at the end of a short  e.g.\n",
    "\n",
    "           cav(e), lov(e), hop(e), crim(e), but\n",
    "           snow, box, tray.\n",
    "        \"\"\"\n",
    "        if i < (self.k0 + 2) or not self.cons(i) or self.cons(i-1) or not self.cons(i-2):\n",
    "            return 0\n",
    "        ch = self.b[i]\n",
    "        if ch == 'w' or ch == 'x' or ch == 'y':\n",
    "            return 0\n",
    "        return 1\n",
    "\n",
    "    def ends(self, s):\n",
    "        \"\"\"ends(s) is TRUE <=> k0,...k ends with the string s.\"\"\"\n",
    "        length = len(s)\n",
    "        if s[length - 1] != self.b[self.k]: # tiny speed-up\n",
    "            return 0\n",
    "        if length > (self.k - self.k0 + 1):\n",
    "            return 0\n",
    "        if self.b[self.k-length+1:self.k+1] != s:\n",
    "            return 0\n",
    "        self.j = self.k - length\n",
    "        return 1\n",
    "\n",
    "    def setto(self, s):\n",
    "        \"\"\"setto(s) sets (j+1),...k to the characters in the string s, readjusting k.\"\"\"\n",
    "        length = len(s)\n",
    "        self.b = self.b[:self.j+1] + s + self.b[self.j+length+1:]\n",
    "        self.k = self.j + length\n",
    "\n",
    "    def r(self, s):\n",
    "        \"\"\"r(s) is used further down.\"\"\"\n",
    "        if self.m() > 0:\n",
    "            self.setto(s)\n",
    "\n",
    "    def step1ab(self):\n",
    "        \"\"\"step1ab() gets rid of plurals and -ed or -ing. e.g.\n",
    "\n",
    "           caresses  ->  caress\n",
    "           ponies    ->  poni\n",
    "           ties      ->  ti\n",
    "           caress    ->  caress\n",
    "           cats      ->  cat\n",
    "\n",
    "           feed      ->  feed\n",
    "           agreed    ->  agree\n",
    "           disabled  ->  disable\n",
    "\n",
    "           matting   ->  mat\n",
    "           mating    ->  mate\n",
    "           meeting   ->  meet\n",
    "           milling   ->  mill\n",
    "           messing   ->  mess\n",
    "\n",
    "           meetings  ->  meet\n",
    "        \"\"\"\n",
    "        if self.b[self.k] == 's':\n",
    "            if self.ends(\"sses\"):\n",
    "                self.k = self.k - 2\n",
    "            elif self.ends(\"ies\"):\n",
    "                self.setto(\"i\")\n",
    "            elif self.b[self.k - 1] != 's':\n",
    "                self.k = self.k - 1\n",
    "        if self.ends(\"eed\"):\n",
    "            if self.m() > 0:\n",
    "                self.k = self.k - 1\n",
    "        elif (self.ends(\"ed\") or self.ends(\"ing\")) and self.vowelinstem():\n",
    "            self.k = self.j\n",
    "            if self.ends(\"at\"):   self.setto(\"ate\")\n",
    "            elif self.ends(\"bl\"): self.setto(\"ble\")\n",
    "            elif self.ends(\"iz\"): self.setto(\"ize\")\n",
    "            elif self.doublec(self.k):\n",
    "                self.k = self.k - 1\n",
    "                ch = self.b[self.k]\n",
    "                if ch == 'l' or ch == 's' or ch == 'z':\n",
    "                    self.k = self.k + 1\n",
    "            elif (self.m() == 1 and self.cvc(self.k)):\n",
    "                self.setto(\"e\")\n",
    "\n",
    "    def step1c(self):\n",
    "        \"\"\"step1c() turns terminal y to i when there is another vowel in the stem.\"\"\"\n",
    "        if (self.ends(\"y\") and self.vowelinstem()):\n",
    "            self.b = self.b[:self.k] + 'i' + self.b[self.k+1:]\n",
    "\n",
    "    def step2(self):\n",
    "        \"\"\"step2() maps double suffices to single ones.\n",
    "        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n",
    "        string before the suffix must give m() > 0.\n",
    "        \"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"ational\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"tional\"):  self.r(\"tion\")\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"enci\"):      self.r(\"ence\")\n",
    "            elif self.ends(\"anci\"):    self.r(\"ance\")\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"izer\"):      self.r(\"ize\")\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"bli\"):       self.r(\"ble\") # --DEPARTURE--\n",
    "            # To match the published algorithm, replace this phrase with\n",
    "            #   if self.ends(\"abli\"):      self.r(\"able\")\n",
    "            elif self.ends(\"alli\"):    self.r(\"al\")\n",
    "            elif self.ends(\"entli\"):   self.r(\"ent\")\n",
    "            elif self.ends(\"eli\"):     self.r(\"e\")\n",
    "            elif self.ends(\"ousli\"):   self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ization\"):   self.r(\"ize\")\n",
    "            elif self.ends(\"ation\"):   self.r(\"ate\")\n",
    "            elif self.ends(\"ator\"):    self.r(\"ate\")\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"alism\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iveness\"): self.r(\"ive\")\n",
    "            elif self.ends(\"fulness\"): self.r(\"ful\")\n",
    "            elif self.ends(\"ousness\"): self.r(\"ous\")\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"aliti\"):     self.r(\"al\")\n",
    "            elif self.ends(\"iviti\"):   self.r(\"ive\")\n",
    "            elif self.ends(\"biliti\"):  self.r(\"ble\")\n",
    "        elif self.b[self.k - 1] == 'g': # --DEPARTURE--\n",
    "            if self.ends(\"logi\"):      self.r(\"log\")\n",
    "        # To match the published algorithm, delete this phrase\n",
    "\n",
    "    def step3(self):\n",
    "        \"\"\"step3() dels with -ic-, -full, -ness etc. similar strategy to step2.\"\"\"\n",
    "        if self.b[self.k] == 'e':\n",
    "            if self.ends(\"icate\"):     self.r(\"ic\")\n",
    "            elif self.ends(\"ative\"):   self.r(\"\")\n",
    "            elif self.ends(\"alize\"):   self.r(\"al\")\n",
    "        elif self.b[self.k] == 'i':\n",
    "            if self.ends(\"iciti\"):     self.r(\"ic\")\n",
    "        elif self.b[self.k] == 'l':\n",
    "            if self.ends(\"ical\"):      self.r(\"ic\")\n",
    "            elif self.ends(\"ful\"):     self.r(\"\")\n",
    "        elif self.b[self.k] == 's':\n",
    "            if self.ends(\"ness\"):      self.r(\"\")\n",
    "\n",
    "    def step4(self):\n",
    "        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n",
    "        if self.b[self.k - 1] == 'a':\n",
    "            if self.ends(\"al\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'c':\n",
    "            if self.ends(\"ance\"): pass\n",
    "            elif self.ends(\"ence\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'e':\n",
    "            if self.ends(\"er\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'i':\n",
    "            if self.ends(\"ic\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'l':\n",
    "            if self.ends(\"able\"): pass\n",
    "            elif self.ends(\"ible\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'n':\n",
    "            if self.ends(\"ant\"): pass\n",
    "            elif self.ends(\"ement\"): pass\n",
    "            elif self.ends(\"ment\"): pass\n",
    "            elif self.ends(\"ent\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'o':\n",
    "            if self.ends(\"ion\") and (self.b[self.j] == 's' or self.b[self.j] == 't'): pass\n",
    "            elif self.ends(\"ou\"): pass\n",
    "            # takes care of -ous\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 's':\n",
    "            if self.ends(\"ism\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 't':\n",
    "            if self.ends(\"ate\"): pass\n",
    "            elif self.ends(\"iti\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'u':\n",
    "            if self.ends(\"ous\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'v':\n",
    "            if self.ends(\"ive\"): pass\n",
    "            else: return\n",
    "        elif self.b[self.k - 1] == 'z':\n",
    "            if self.ends(\"ize\"): pass\n",
    "            else: return\n",
    "        else:\n",
    "            return\n",
    "        if self.m() > 1:\n",
    "            self.k = self.j\n",
    "\n",
    "    def step5(self):\n",
    "        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if\n",
    "        m() > 1.\n",
    "        \"\"\"\n",
    "        self.j = self.k\n",
    "        if self.b[self.k] == 'e':\n",
    "            a = self.m()\n",
    "            if a > 1 or (a == 1 and not self.cvc(self.k-1)):\n",
    "                self.k = self.k - 1\n",
    "        if self.b[self.k] == 'l' and self.doublec(self.k) and self.m() > 1:\n",
    "            self.k = self.k -1\n",
    "\n",
    "    def stem(self, p, i=0, j=None):\n",
    "        \"\"\"In stem(p,i,j), p is a char pointer, and the string to be stemmed\n",
    "        is from p[i] to p[j] inclusive. Typically i is zero and j is the\n",
    "        offset to the last character of a string, (p[j+1] == '\\0'). The\n",
    "        stemmer adjusts the characters p[i] ... p[j] and returns the new\n",
    "        end-point of the string, k. Stemming never increases word length, so\n",
    "        i <= k <= j. To turn the stemmer into a module, declare 'stem' as\n",
    "        extern, and delete the remainder of this file.\n",
    "        \"\"\"\n",
    "        # copy the parameters into statics\n",
    "        if j == None:\n",
    "            j = len(p)-1\n",
    "        self.b = p\n",
    "        self.k = j\n",
    "        self.k0 = i\n",
    "        if self.k <= self.k0 + 1:\n",
    "            return self.b # --DEPARTURE--\n",
    "\n",
    "        # With this line, strings of length 1 or 2 don't go through the\n",
    "        # stemming process, although no mention is made of this in the\n",
    "        # published algorithm. Remove the line to match the published\n",
    "        # algorithm.\n",
    "\n",
    "        self.step1ab()\n",
    "        self.step1c()\n",
    "        self.step2()\n",
    "        self.step3()\n",
    "        self.step4()\n",
    "        self.step5()\n",
    "        return self.b[self.k0:self.k+1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
